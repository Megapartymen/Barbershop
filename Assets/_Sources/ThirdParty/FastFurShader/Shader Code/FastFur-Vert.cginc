// The vertex shader calculates the camera distance to the mesh, which determines:
//   - How many layers of fur are visible
//   - How far apart the layers are
//   - How much detail is visible (ie. "FURFADEIN")
// 
// The vertex shader is also responsible for determining where the skin layer is.
// At far distances only the skin layer will be visible, but as the camera moves closer
// the skin layer will shrink and be replaced with layers of fur generated by the geometry shader.
//
// The vertex shader also pre-calculates as much lighting as possible. This isn't as
// accurate as doing it all in the fragment shader, but it's faster.

#include "FastFur-Function-Wind.cginc"


#if defined(SKIN_LAYER)
fragInput vert(meshData v)
{
	fragInput o = (fragInput)0;
#else
hullGeomInput vert(meshData v)
{
	hullGeomInput o = (hullGeomInput)0;
#endif



	//--------------------------------------------------------------------------------
	// Get height samples
	// In theory, getting these first gives the samplers time to get the values while we are doing other calculations.
	// Apparently this helps on AMD chipsets, but NVidia and Intel claim it makes no difference for theirs due to automatic optimization.
	float2 uv = _SelectedUV == 3 ? v.uv3 : _SelectedUV == 2 ? v.uv2 : _SelectedUV == 1 ? v.uv1 : v.uv0;
	float4 furShape = _FurShapeMap.SampleLevel(my_point_repeat_sampler, uv, 0);
	float4 furMask1 = float4(1, 1, 1, 1);
	float4 furMask2 = float4(1, 1, 1, 1);
	if (_FurShapeMask1Bits > 0) furMask1 = _FurShapeMask1.SampleLevel(my_point_repeat_sampler, uv, 0);
	if (_FurShapeMask2Bits > 0) furMask2 = _FurShapeMask2.SampleLevel(my_point_repeat_sampler, uv, 0);



	// Eventually, I want to get this working. The idea is to make the hairs a bit shorter when they are combed,
	// so that they have a consistent length. There's still 'something' off about the calculations, though.
	/*    if (_FurCombCompression > 0)
		{
			float furCombingLength = length(.498 - furShape.xy);
			thicknessSample *= rsqrt(pow(furCombingLength * _FurCombStrength * _FurShellSpacing * _FurCombCompression * 10, 2) + 1);
		}*/



		//--------------------------------------------------------------------------------
		// Single-Pass Stereo Instancing support
		//
		// This is a good place to put my notes about how this works. The Unity documentation is incomplete, and
		// doesn't describe what to do with hull or geometry shaders. Likewise, most user-created guides typically
		// just say, "do this", for a particular use case, and don't cover some "why" details that I need for my shader.
		//
		// So here is what I know. First, there are 2 different macros that need to be included in the data structures.
		// The first one is UNITY_VERTEX_INPUT_INSTANCE_ID, which needs to go into the input data for the vertex shader.
		// Unity will change what these macros do as-needed, but typically when SPSI is active they do the following:
		// 
		// UNITY_VERTEX_INPUT_INSTANCE_ID                  ->   uint instanceID : SV_InstanceID;
		//
		// Second, the input data of every stage after the vertex shader needs to contain UNITY_VERTEX_OUTPUT_STEREO:
		// 
		// UNITY_VERTEX_OUTPUT_STEREO                      ->   uint stereoTargetEyeIndex : SV_RenderTargetArrayIndex;
		//
		//
		// Now comes the processing. At the start of the vertex shader we need to use UNITY_SETUP_INSTANCE_ID(v), which
		// extracts the information it needs from the input data (exactly how I'm not showing here, because there are a
		// lot of variations) and stores it in variables for use later:
		//
		// UNITY_SETUP_INSTANCE_ID(v)                      ->   unity_StereoEyeIndex = inputInstanceID & 0x01;
		//                                                      unity_InstanceID = unity_BaseInstanceID + (inputInstanceID >> 1);
		//
		//
		// This macro makes sure everything is initialized with 0:
		// 
		// UNITY_INITIALIZE_OUTPUT(type,name)              ->   name = (type)0;
		//
		//
		// We then need to put the data into the output data structure:
		//
		// UNITY_INITIALIZE_VERTEX_OUTPUT_STEREO(output)   ->   output.stereoTargetEyeIndex = unity_StereoEyeIndex;
		//
		//
		// So that takes care of the vertex shader, but what about the hull, domain, and geometry shaders? In my case,
		// the hull and domain shaders are copying whole data structures, so the stereoTargetEyeIndex data is getting
		// copied to the next stage automatically. I don't need to use any special macros for them.
		// 
		// The geomtery shader has to convert from hullGeomInput to fragInput, so it does need to use a macro to ensure
		// the data gets copied:
		// 
		// UNITY_TRANSFER_VERTEX_OUTPUT_STEREO(input, output) ->   output.stereoTargetEyeIndex = input.stereoTargetEyeIndex;
		//
		// Finally, both the geometry and fragment shaders may need to know the unity_StereoEyeIndex, because some of the macros
		// they use may need it. So we add this macro at the start of both of them:
		// 
		// UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input)    ->   unity_StereoEyeIndex = input.stereoTargetEyeIndex;
		// 

	UNITY_SETUP_INSTANCE_ID(v);
#if defined(SKIN_LAYER)
	UNITY_INITIALIZE_OUTPUT(fragInput, o);
	UNITY_INITIALIZE_VERTEX_OUTPUT_STEREO(o);
#else
	UNITY_INITIALIZE_OUTPUT(hullGeomInput, o);
	UNITY_INITIALIZE_VERTEX_OUTPUT_STEREO(o);
#endif



	//--------------------------------------------------------------------------------
	// Determine render quality. There are 9 options:
	//    _V4QualityEditor
	//    _V4QualityVR
	//    _V4Quality2D
	//    _V4QualityVRMirror
	//    _V4Quality2DMirror
	//    _V4QualityCameraView
	//    _V4QualityStreamCamera
	//    _V4QualityCameraPhoto
	//    _V4QualityScreenshot
	//
	// _VRChatCameraMode
	//    0 - Rendering normally
	//    1 - Rendering in VR handheld camera
	//    2 - Rendering in Desktop handheld camera
	//    3 - Rendering for a screenshot
	//
	// _VRChatMirrorMode
	//    0 - Rendering normally, not in a mirror
	//    1 - Rendering in a mirror viewed in VR
	//    2 - Rendering in a mirror viewed in desktop mode

	// If we are rendering in stereo, then the quality is always _V4QualityVR. Mirrors, cameras, etc..., aren't actually rendered in stereo, even if we are in VR while looking at them.
	// Since this is handled by a shader keyword, this means VR will be slightly faster than the other options
#if defined (USING_STEREO_MATRICES)
	float quality = _V4QualityVR;
#else
	float quality = _VRChatCameraMode < -0.5 ? _V4QualityEditor : _V4Quality2D;
	if (_VRChatCameraMode > 2.5)
	{
		quality = _V4QualityScreenshot;
	}
	else if (_VRChatCameraMode > 0.5)
	{
		quality = (_ScreenParams.y == 720) ? _V4QualityCameraView : (_ScreenParams.y == 1080 || _ScreenParams.y == 1440 || _ScreenParams.y == 2160 || _ScreenParams.y == 4320) ? _V4QualityCameraPhoto : _V4QualityStreamCamera;
	}
	else if (_VRChatMirrorMode > 0.5)
	{
		quality = _VRChatMirrorMode > 1.5 ? _V4Quality2DMirror : _V4QualityVRMirror;
	}
#endif
	// Bump "Fastest" up to 100, and "Very Fast" up to 125
	quality += saturate(2 - quality) * 0.5;
	quality = max(0, min(15.5, quality + _OverrideQualityBias * 0.02));


	//--------------------------------------------------------------------------------
	// Convert coordinates to world space
	o.worldPos.xyz = mul(unity_ObjectToWorld, v.vertex);
	o.worldNormal.xyz = UnityObjectToWorldNormal(v.normal);
	o.uv.xy = uv;
#if defined (USING_STEREO_MATRICES)
	float3 viewVector = (unity_StereoWorldSpaceCameraPos[0].xyz + unity_StereoWorldSpaceCameraPos[1].xyz) * 0.5 - o.worldPos.xyz;
#else
	float3 viewVector = _WorldSpaceCameraPos.xyz - o.worldPos.xyz;
#endif
	float3 viewDir = normalize(viewVector);
	float viewDistance = max(0, length(viewVector) + _OverrideDistanceBias);
#if !defined(SKIN_LAYER)
	o.VIEWDISTANCE = viewDistance;
#endif



	// How thick is the fur in world space?
	float worldThickness = 0.95 * length(mul((float3x3) unity_ObjectToWorld, v.normal)) * _ScaleCalibration * _OverrideScale * _FurShellSpacing;
	float distanceToFur = max(0.001, distance(o.worldPos.xyz, _WorldSpaceCameraPos) + _OverrideDistanceBias + 0.05);



	// We've done as many calculations as possible before needing to know the fur thickness.
    // Again, the theory here is that AMD benefits slightly by giving the GPU time to get
    // the thickness sample, while Nvidia and Intel optimizes the order of code execution
    // so that this isn't necessary. It may be that AMD now does the same, and the info
    // is out-of-date.



	//--------------------------------------------------------------------------------
	// Apply optional height masks
	if (_FurShapeMask1Bits > 0)
	{
		if (_FurShapeMask1Bits & 1) furShape.z = min(furShape.z, furMask1.x);
		if (_FurShapeMask1Bits & 2) furShape.z = min(furShape.z, furMask1.y);
		if (_FurShapeMask1Bits & 4) furShape.z = min(furShape.z, furMask1.z);
		if (_FurShapeMask1Bits & 8) furShape.z = min(furShape.z, furMask1.w);
	}
	if (_FurShapeMask2Bits > 0)
	{
		if (_FurShapeMask2Bits & 1) furShape.z = min(furShape.z, furMask2.x);
		if (_FurShapeMask2Bits & 2) furShape.z = min(furShape.z, furMask2.y);
		if (_FurShapeMask2Bits & 4) furShape.z = min(furShape.z, furMask2.z);
		if (_FurShapeMask2Bits & 8) furShape.z = min(furShape.z, furMask2.w);
	}
	float thicknessSample = furShape.z < _FurMinHeight ? 0.0 : furShape.z;
	float maxThickness = worldThickness * thicknessSample;



	//--------------------------------------------------------------------------------
	// Calculate the apparent size of the fur. Use that to determine how many shells
	// need to be rendered, and the amount of details to fade-in.

	float clampedThickness = max((min(maxThickness, 0.025) + 0.025) * 0.5, quality > 14.9 ? maxThickness : (min(maxThickness, 0.05) + min(maxThickness, 0.035)) * 0.5);
	float furApparentSize = clampedThickness / distanceToFur;
	float shellDensity = BASE_SHELL_COUNT * LAYER_DENSITY * (1 + quality);
	float shellsToRender = furApparentSize * shellDensity;

	// Are we past the maximum distance?
	float maxDistance = max(1, min(10, clampedThickness * shellDensity));
	shellsToRender *= viewDistance > maxDistance ? 0 : 1;

	o.FURFADEIN = saturate(invLerp(maxDistance, 0.3, max(0.3, distanceToFur)));



	//--------------------------------------------------------------------------------
	// Calculate skin position
	float skinZ = 0;
#if defined(SKIN_LAYER)
	if (_BodyExpansion > 0 || _BodyShrinkOffset > 0) {
#endif
		float resizeFactor = saturate(thicknessSample - _BodyResizeCutoff) / (1 - _BodyResizeCutoff);

		// Calculate a negative offset in world space to the skin position based on the fur height
		float3 bodyShrink = o.worldNormal * _BodyShrinkOffset * maxThickness * -0.40;
		o.worldPos.xyz += bodyShrink;

		// Calculate where the skin should be (0-1) relative to the thickness of the fur
		float skinFactor = saturate(1 - (o.FURFADEIN * o.FURFADEIN)) * SKIN_CUTOFF * _BodyExpansion;
		skinZ = skinFactor * resizeFactor;
#if defined(SKIN_LAYER)
	}
#endif



	//--------------------------------------------------------------------------------
	// Process normal map
#if !defined (PREPASS)
	float4 worldTangent = float4(UnityObjectToWorldDir(v.tangent.xyz), v.tangent.w);
	float3 worldBinormal = cross(o.worldNormal.xyz, worldTangent.xyz) * (worldTangent.w * unity_WorldTransformParams.w);


#if defined(SKIN_LAYER) && defined(_NORMALMAP)
	if (_PBSSkin > 0.0)
	{
		o.worldTangent = worldTangent;
	}
#endif
#endif


	//--------------------------------------------------------------------------------
	// Calculate wind
	float3 windVector = 0;
	if (_EnableWind > 0 && _WindSpeed > 0) windVector = calculateWind(o.worldPos.xyz, uv.xy);

	// Apply the effects of movement as additional wind
	if (_MovementStrength > 0)
	{
		windVector += mul(unity_ObjectToWorld, float4(float3(_VelocityX, _VelocityY, _VelocityZ) * _MovementStrength, 0)) * 0.25;
	}
#if !defined(SKIN_LAYER)
	o.windEffect.xyz = windVector;
#endif


	//--------------------------------------------------------------------------------
	// Vertex lighting

	// If the fur is facing the camera, it won't have sub-surface scattering
	float scatterAtten = 0;
	if (_SubsurfaceScattering > 0) scatterAtten = saturate(dot(viewDir, o.worldNormal.xyz));


	// "Important" world space lights
#if !defined (PREPASS)
	float lightOn = _LightColor0.a;
	float3 lightDir = _WorldSpaceLightPos0.xyz;
	if (lightOn == 0 && _FallbackLightEnable > 0)
	{
		float lightYCos = cos(radians(_FallbackLightAngle));
		lightDir = normalize(float3(sin(radians(_FallbackLightDirection)) * lightYCos, -sin(radians(_FallbackLightAngle)), cos(radians(_FallbackLightDirection)) * lightYCos));
		lightOn = 1;
	}
	if (lightOn > 0)
	{
#if defined(POINT) || defined(SPOT)
		lightDir = normalize(lightDir - o.worldPos.xyz);
#endif


		// Diffuse (Lambertian)
		o.MAINLIGHTDOT = dot(o.worldNormal.xyz, lightDir);


		// Anisotropic reflections. This isn't even close to being correct, but doing things correctly means
		// doing it mostly in the fragment shader, which cuts frame rate by about 1/3, so it's not an option.
		if (_FurAnisotropicEnable > 0)
		{
			// Get the direction that the hair is pointing.
			float3 hairVector = (float3(furShape.xy, 0) * 2 - 1) * _FurCombStrength;
			hairVector.z = sqrt(1 - dot(hairVector.xy, hairVector.xy));
			hairVector = normalize(hairVector.x * worldTangent + hairVector.y * worldBinormal + hairVector.z * normalize(o.worldNormal.xyz) * (1.001 - _FurAnisoFlat));
			hairVector = normalize(hairVector - ((windVector * _FurAnisoWindShimmer) + float3(0, _FurGravitySlider, 0)));

			// If the light hits the hair at right angle, it reflects outwards in a disc.
			// As it hits at steeper angles, that disc turns into a cone. However, the hair
			// also has a prism-like structure, so it will add an offset to the angle of reflection.
			// Combining these two factors gives us a cone-shaped angle. The closer the view
			// direction is to this angle, the brighter the reflection/refraction.

			// Start by getting the dot product of the hair and the light vectors. If the light
			// is hitting at a right angle, the result will be 0. If it is hitting the hair straight
			// on the result will be -1.
			float lightDot = dot(hairVector, lightDir);

			// The light will reflect off at an angle that is biased towards the hair tip due to
			// the structure of the hair. The property sliders control the amount of bias. We are
			// just adding a linear bias, which isn't correct, but it's simple to calculate.
			float2 anisoAngle = lightDot + float2(_FurAnisoReflectAngle, _FurAnisoRefractAngle) * 0.01111;

			// Now get the dot product of the hair and the view angle. If the negative of the product
			// (negative because we are viewing from the opposite direction) is close to of the
			// reflect/refract angles, then the light will be visible.
			float hairAndViewDot = -dot(hairVector, -viewDir);

			// Glossiness determines how wide the possible view angle difference is.
			float2 anisoGloss = float2(_FurAnisoReflectGloss, _FurAnisoRefractGloss);
			float2 anisoSetting = float2(_FurAnisotropicReflect, _FurAnisotropicRefract);
			float2 glossFactor = 0.35 + (anisoGloss * anisoGloss);
			float2 anisoStrength = saturate((1 - abs(anisoAngle - hairAndViewDot)) - (0.65 + anisoGloss * 0.2)) * glossFactor;
			o.ANISOTROPICBOTH = (anisoStrength * anisoSetting);
			o.ANISOTROPICANGLE = (anisoAngle.x - hairAndViewDot) * glossFactor.x;
		}


		// Simulate subsurface scattering. This isn't accurate at all, but accurate != fast.
		// The basic idea is that if the viewer is behind the avatar, facing towards the light,
		// light will be visible from fur that is perpendicular to the viewer.
		if (_SubsurfaceScattering > 0)
		{
			float scatterStrength = dot(viewDir, -lightDir);// Is the camera looking past our position and into the light?
			scatterStrength -= scatterAtten * 1.25;// Is the fur facing the camera? If so, the light gets blocked.

			o.SUBSURFACESTRENGTH = saturate(scatterStrength) * _SubsurfaceScattering;
		}
	}


#if defined(FORWARD_BASE_PASS)
	float oneMinusSoftenShadows = 1.0 - (_SoftenShadows * 0.1);
#endif


	// Apply the 4 vertex lights (which are always point lights). This code is copy-pasted from UnityCG.cginc, but has been
	// modified to optionally re-mix some of the light as ambient light.
#if defined(VERTEXLIGHT_ON)
	// to light vectors
	float4 toLightX = unity_4LightPosX0 - o.worldPos.x;
	float4 toLightY = unity_4LightPosY0 - o.worldPos.y;
	float4 toLightZ = unity_4LightPosZ0 - o.worldPos.z;
	// squared lengths
	float4 lengthSq = 0;
	lengthSq += toLightX * toLightX;
	lengthSq += toLightY * toLightY;
	lengthSq += toLightZ * toLightZ;
	// don't produce NaNs if some vertex position overlaps with the light
	lengthSq = max(lengthSq, 0.000001);

	// NdotL
	float4 ndotl = 0;
	ndotl += toLightX * o.worldNormal.x;
	ndotl += toLightY * o.worldNormal.y;
	ndotl += toLightZ * o.worldNormal.z;
	// correct NdotL
	float4 corr = rsqrt(lengthSq);
	ndotl = max(float4(0, 0, 0, 0), ndotl * corr);
	// attenuation
	float4 atten = 1.0 / (1.0 + lengthSq * unity_4LightAtten0);
	float4 diff = (ndotl * atten * oneMinusSoftenShadows) + (atten * (_SoftenShadows * 0.1));
	// final color
	o.lightData1.rgb += unity_LightColor[0].rgb * diff.x;
	o.lightData1.rgb += unity_LightColor[1].rgb * diff.y;
	o.lightData1.rgb += unity_LightColor[2].rgb * diff.z;
	o.lightData1.rgb += unity_LightColor[3].rgb * diff.w;

	if (_SubsurfaceScattering > 0)
	{
		float4 cameraToLightX = _WorldSpaceCameraPos.x - unity_4LightPosX0;
		float4 cameraToLightY = _WorldSpaceCameraPos.y - unity_4LightPosY0;
		float4 cameraToLightZ = _WorldSpaceCameraPos.z - unity_4LightPosZ0;
		float4 scatterStrength = 0;
		scatterStrength += cameraToLightX * viewDir.x;
		scatterStrength += cameraToLightY * viewDir.y;
		scatterStrength += cameraToLightZ * viewDir.z;
		scatterStrength -= scatterAtten * 3;// Is the fur facing the camera? If so, the light gets blocked.

		diff = max(0, (scatterStrength * atten * oneMinusSoftenShadows) + (atten * (_SoftenShadows * 0.1))) * 0.4;
		o.lightData1.rgb += unity_LightColor[0].rgb * diff.x;
		o.lightData1.rgb += unity_LightColor[1].rgb * diff.y;
		o.lightData1.rgb += unity_LightColor[2].rgb * diff.z;
		o.lightData1.rgb += unity_LightColor[3].rgb * diff.w;
	}
#endif


	// Optionally re-mix some of the 'important' light into ambient
	// This doesn't handle SPOT or POINT lights because those require a texture
	// sample to calculate the attentuation, which isn't worth the FPS cost.
#if defined(DIRECTIONAL)
	o.lightData1.rgb += min(_LightColor0, _MaxBrightness) * (_SoftenShadows * 0.1);
#endif


	// Spherical harmonics lights
#if defined(FORWARD_BASE_PASS)
	o.lightData1.rgb += ShadeSH9(float4(o.worldNormal.xyz, 1));
	o.lightData1.rgb += (_SoftenShadows * 0.4) * ShadeSH9(float4(-o.worldNormal.xyz, 1));
	o.lightData1.rgb = min(o.lightData1.rgb, _MaxBrightness);

	if (_SubsurfaceScattering > 0)
	{
		float diff = max(0, ((1 - (scatterAtten * 3)) * oneMinusSoftenShadows) + (_SoftenShadows * 0.1)) * 0.4;
		o.lightData1.rgb += ShadeSH9(float4(-viewDir, 1)).rgb * diff * _SubsurfaceScattering;
	}


	if (_WorldLightReColourStrength > 0)
	{
		float brightness = (o.lightData1.r * 0.30 + o.lightData1.g * 0.59 + o.lightData1.b * 0.11);
		o.lightData1.rgb = (o.lightData1.rgb * (1 - _WorldLightReColourStrength)) + (_WorldLightReColour.rgb * _WorldLightReColourStrength * brightness);
	}

	// Extra Rim/Front lighting
	if (_ExtraLightingEnable > 0)
	{
		float strength = _ExtraLighting;
		if (_ExtraLightingRim != 0)
		{
			float dotView = dot(o.worldNormal.xyz, viewDir);

			if (_ExtraLightingRim > 0)
			{
				strength *= pow(abs(dot(o.worldNormal.xyz, viewDir)), _ExtraLightingRim);
			}
			else
			{
				strength *= pow(1 - abs(dot(o.worldNormal.xyz, viewDir)), 0 - _ExtraLightingRim);
			}
		}
		if (_ExtraLightingMode > 0)
		{
			o.lightData1.rgb = max(o.lightData1.rgb, _ExtraLightingColor * strength);
		}
		else
		{
			o.lightData1.rgb += _ExtraLightingColor * strength;
		}
	}
#endif
#endif


	//--------------------------------------------------------------------------------
	// Process shadows
#if defined(USE_SHADOWS) && (defined(SHADOWS_SCREEN) || defined(SHADOWS_DEPTH) || defined(SHADOWS_CUBE))
   // At extremly close ranges we need to shrink the shadows, otherwise the camera will be inside the shadow casting radius, which will cause glitches
	float shadowHeight = 0.95 * thicknessSample * _ScaleCalibration * _OverrideScale * _FurShellSpacing * _FurShadowCastSize * saturate((viewDistance - 0.035) * 20);
	shadowStruct tempOut;
	tempOut.pos = UnityObjectToClipPos(v.vertex + v.normal.xyz * shadowHeight);
	TRANSFER_SHADOW(tempOut);
	o._ShadowCoord = tempOut._ShadowCoord;
#endif


	//--------------------------------------------------------------------------------
	// Process skin layer (then exit)
#if defined(SKIN_LAYER)
   // Note that the skin layer packs the ZDATA differently than the fur layers.
   // It just packs the skin position without any fancy modulus stuf...
	o.ZDATA = skinZ;

	float bend = pow(_HairStiffness * 0.85 + ((1 - _HairStiffness) * skinZ), 2);
	float3 adjustedScaledNormal = normalize(o.worldNormal.xyz - ((saturate(o.FURFADEIN * 5.0) * windVector) + float3(0, _FurGravitySlider, 0)) * bend) * worldThickness;

	o.worldPos.xyz += adjustedScaledNormal * skinZ;
	o.pos = UnityWorldToClipPos(o.worldPos.xyz);
	UNITY_TRANSFER_FOG(o, o.pos);
	return o;
#else


	//--------------------------------------------------------------------------------
	// Process fur layers
	o.WORLDTHICKNESS = worldThickness;
	o.SKIN_Z = skinZ;


	//--------------------------------------------------------------------------------
	// Where is this vertex on the visible screen? 0 = centre, >1 = off-screen
	float3 firstOffset = o.worldNormal.xyz * worldThickness * thicknessSample;
	float3 secondOffset = normalize(o.worldNormal.xyz - (o.windEffect.xyz + float3(0, _FurGravitySlider, 0))) * worldThickness * thicknessSample;
	// Create a triangle where one corner is the root position, the second is where the hair tip would be if there
	// was no wind/gravity, and the third is the actual hair tip location. The Hull shader will use this to determine
	// if the resulting triangle-of-triangles is visible or not.
	float4 pos0 = UnityWorldToClipPos(o.worldPos.xyz);
	float4 pos1 = UnityWorldToClipPos(o.worldPos.xyz + firstOffset);
	float4 pos2 = UnityWorldToClipPos(o.worldPos.xyz + secondOffset);
	float3 screenPos0 = pos0.xyz / pos0.w;
	float3 screenPos1 = pos1.xyz / pos1.w;
	float3 screenPos2 = pos2.xyz / pos2.w;
	o.screenPosMin = min(HULL_SCREEN_CULLING, max(-HULL_SCREEN_CULLING, min(min(screenPos0.xy, screenPos1.xy), screenPos2.xy)));
	o.screenPosMax = min(HULL_SCREEN_CULLING, max(-HULL_SCREEN_CULLING, max(max(screenPos0.xy, screenPos1.xy), screenPos2.xy)));


	// The FURCULLTEST is used by the hull shader to throw out backwards geometry. We also use it to identify edge geometry so that
	// we can enhance it.
	// -1 = Directly facing the camera, 1 = Directly facing away from the camera
	o.FURCULLTEST = dot(o.worldNormal.xyz, normalize(o.worldPos.xyz - _WorldSpaceCameraPos));


	// If we are out of range, or near zero-height, then mark that we don't want any layers and exit
	if (shellsToRender < 0.33 || (thicknessSample - ((1 - o.FURFADEIN) * 0.15) <= 0))
	{
		o.VISIBLELAYERS = 0;
		return o;
	}


	//--------------------------------------------------------------------------------
	// Determine how many shells to render, based on density.
#if defined(PIPELINE3)
#if defined(LITEFUR)
	float maxShells = 16 + max(0, quality - 1) * 8;
#else
	float maxShells = 32 + max(0, quality - 1) * 16;
#endif
#else
	float maxShells = GEOM_STOP_SHELL;
#endif


	// Apply the middle-limit. This is to prevent extreme, face-in-floof close-ups from turning into slide-shows.
	shellsToRender = min(shellsToRender, MID_SHELLS_FACTOR * maxShells);


	// Gradually enhance all fur towards edges (reduces dot-crawl when zooming in/out and reduces visible gaps in the edges of the fur)
	shellsToRender *= 4.0 - (4.0 * (saturate(abs(o.FURCULLTEST) + 0.25) - 0.25));


	// Apply the max limit
	shellsToRender = min(shellsToRender, maxShells - MIN_SHELLS);


	// If the fur has been compressed because it's below half-height, remove some layers so that the resulting
	// density is half-way between the "normal" density and the compressed density.
	if (shellsToRender > 2) shellsToRender = lerp(2, shellsToRender, saturate(0.5 + thicknessSample));


#if defined (USING_STEREO_MATRICES)
	// If we are in VR, then we can gradually lower the resolution near the edges, thanks to the fact that
	// current VR lenses kinda suck and everything gets blurry away from the centre.
	shellsToRender *= max(0.5, saturate(1.35 - length(screenPos0)));
#endif


	o.VISIBLELAYERS = MIN_SHELLS + shellsToRender;
	o.MAX_Z = thicknessSample;

	return o;
#endif
}
